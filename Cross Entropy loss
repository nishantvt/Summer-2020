Below is the general snippet for computation of forward cross-entropy loss in Multi class Classification


def forward(self, bottom, top):
   labels = bottom[1].data
   scores = bottom[0].data
   
   # Normalizing to avoid instability
   scores -= np.max(scores, axis=1, keepdims=True)  
   
   # Compute Softmax activations 
   exp_scores = np.exp(scores)
   probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True) 
   logprobs = np.zeros([bottom[0].num,1])
   
   # Compute cross-entropy loss
   for N in range(bottom[0].num): # For each element in the batch
       scale_factor = 1 / float(np.count_nonzero(labels[N, :]))
       for c in range(len(labels[N,:])): # For each class 
           if labels[N,c] != 0:  # Positive classes
               logprobs[N] += -np.log(probs[N,c]) * labels[N,c] * scale_factor     #We sum the loss per class for each element of the batch

   data_loss = np.sum(logprobs) / bottom[0].num

   self.diff[...] = probs  # Store softmax activations
   top[0].data[...] = data_loss # Store loss

